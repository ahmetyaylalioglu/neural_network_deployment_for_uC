{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Part of the training and accuracy check were taken from CS231N assigment-2\n",
    "\n",
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "def check_accuracy_part(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc\n",
    "def train_part(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on MNIST using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    best_acc = -1\n",
    "    best_model = None\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(data_train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                acc = check_accuracy_part(data_test_loader, model)\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_model = model\n",
    "                print()\n",
    "    print('Best accuracy found', best_acc)\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "if 0:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n",
      "Iteration 0, loss = 2.2982\n",
      "Checking accuracy on test set\n",
      "Got 1135 / 10000 correct (11.35)\n",
      "\n",
      "Iteration 50, loss = 0.3992\n",
      "Checking accuracy on test set\n",
      "Got 8850 / 10000 correct (88.50)\n",
      "\n",
      "Iteration 100, loss = 0.2321\n",
      "Checking accuracy on test set\n",
      "Got 9344 / 10000 correct (93.44)\n",
      "\n",
      "Iteration 150, loss = 0.1519\n",
      "Checking accuracy on test set\n",
      "Got 9563 / 10000 correct (95.63)\n",
      "\n",
      "Iteration 200, loss = 0.1608\n",
      "Checking accuracy on test set\n",
      "Got 9588 / 10000 correct (95.88)\n",
      "\n",
      "Best accuracy found 0.9588\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Flatten()\n",
      "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if 0:\n",
    "    # Example network on CIFAR10 dataset\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    trainset = dset.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    data_train_loader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                              shuffle=True)\n",
    "\n",
    "    testset = dset.CIFAR10(root='./data', train=False,\n",
    "                                           download=True, transform=transform)\n",
    "    data_test_loader = torch.utils.data.DataLoader(testset, batch_size=512,\n",
    "                                             shuffle=False)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    print_every = 50\n",
    "    print('using device:', device)\n",
    "\n",
    "    # Neural network architecture\n",
    "    # Current code only supports conv2d-ReLU-maxPool2d pairs together and Linear\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "        nn.Conv2d(32, 16, kernel_size=5, stride=1, padding=2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "        nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "        Flatten(),\n",
    "\n",
    "        nn.Linear(4*4*32, 10),\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # Example network(Lenet-5) on MNIST dataset\n",
    "    data_train = MNIST('./data/mnist',\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize((32, 32)),\n",
    "                           transforms.ToTensor()]))\n",
    "\n",
    "    data_test = MNIST('./data/mnist',\n",
    "                      train=False,\n",
    "                      download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize((32, 32)),\n",
    "                          transforms.ToTensor()]))\n",
    "\n",
    "    data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True)\n",
    "    data_test_loader = DataLoader(data_test, batch_size=1024)\n",
    "\n",
    "\n",
    "    print_every = 50\n",
    "    print('using device:', device)\n",
    "\n",
    "\n",
    "    # Lenet-5 architecture\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "        nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "        Flatten(),\n",
    "        nn.Linear(5*5*16, 120),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, 10),\n",
    "    )    \n",
    "    \n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=2e-2, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "\n",
    "best_model = train_part(model, optimizer, 1)\n",
    "\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.9673, -4.0996,  0.5660, -5.6191, 12.4658, -1.8248,  1.4571,  3.1900,\n",
      "        -2.9620,  2.4343], grad_fn=<SelectBackward>) \n",
      " prediction: tensor(4)\n",
      "Conv layer\n",
      "w_0_weight_2d (6, 1, 5, 5)\n",
      "Conv layer\n",
      "w_3_weight_2d (16, 6, 5, 5)\n",
      "linear layer\n",
      "w_7_weight_2d (120, 400)\n",
      "linear layer\n",
      "w_9_weight_2d (84, 120)\n",
      "linear layer\n",
      "w_11_weight_2d (10, 84)\n"
     ]
    }
   ],
   "source": [
    "# weights header and network generator. This generates main.h and main.c\n",
    "weights_file = open('main.h', 'w')\n",
    "weights_file.write('typedef float data_t;\\n\\n')\n",
    "\n",
    "c_file = open('main.c', 'w')\n",
    "c_file.write('#include <stdio.h>\\n\\\n",
    "#include <string.h>\\n\\\n",
    "#include <stdint.h>\\n\\\n",
    "#include \"main.h\"\\n\\\n",
    "#include \"helper_functions.h\"\\n\\\n",
    "\\n')\n",
    "\n",
    "c_file.write('\\n\\\n",
    "int main()\\n\\\n",
    "{\\n')\n",
    "\n",
    "\n",
    "test_vector_batch = 5\n",
    "test_vector_index_in_batch = 6\n",
    "\n",
    "test_vector = None\n",
    "\n",
    "for i, (images, labels) in enumerate(data_test_loader):\n",
    "    if i == test_vector_batch:\n",
    "        test_vector_index_in_batch = 3\n",
    "\n",
    "        img = images[test_vector_index_in_batch].numpy()\n",
    "        #print(img.shape)\n",
    "        #plt.imshow(img.reshape(32,32), vmin=0, vmax=1)\n",
    "        #plt.imshow(img.transpose(1,2,0), vmin=0, vmax=1)\n",
    "        \n",
    "        test_vector = images\n",
    "        \n",
    "        weights_file.write('const data_t test['+str(img.size)+']={')\n",
    "        np.savetxt(weights_file, images[test_vector_index_in_batch].flatten(), newline=',')\n",
    "        weights_file.write('};\\n')\n",
    "\n",
    "        images = images.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        #output = intermediate_network(images)\n",
    "        output = best_model(images)\n",
    "        #print(output.shape)\n",
    "        _, preds = output.max(axis=1)\n",
    "        #print(output[test_vector_index_in_batch, 0])\n",
    "        print(output[test_vector_index_in_batch], '\\n prediction:', preds[test_vector_index_in_batch])\n",
    "        break\n",
    "\n",
    "result = test_vector\n",
    "input_size = result.shape[1]*result.shape[2]*result.shape[3]\n",
    "L = np.empty(0)\n",
    "L = np.append(L, np.uint32(input_size))\n",
    "\n",
    "#print(result.shape[1], result.shape[2], result.shape[3])\n",
    "\n",
    "previous_padding = None\n",
    "\n",
    "index = 0\n",
    "\n",
    "meta_list = list()\n",
    "\n",
    "c_file.write('\\t twoD_t meta_data'+str(index)+' = {\\n\\\n",
    "                           .r = '+ str(result.shape[2]) +',\\n\\\n",
    "                           .c = '+str(result.shape[3])+',\\n\\\n",
    "                           .channel = '+str(result.shape[1])+',\\n\\\n",
    "                           .data = buffer'+str(index%2)+',\\n\\\n",
    "                           .bias = NULL\\n\\\n",
    "                       };\\n\\n')\n",
    "\n",
    "meta_list.append(('meta_data'+str(index),(0,0,0)))\n",
    "\n",
    "for i in best_model:\n",
    "    result = i(result)\n",
    "    if 'Conv2d' in str(i):\n",
    "        previous_padding = i.padding[0]\n",
    "    if 'ReLU' in str(i):\n",
    "        continue\n",
    "    if 'MaxPool' in str(i):\n",
    "        index += 1\n",
    "        #print(i.stride, i.kernel_size)\n",
    "        L = np.append(L, result.shape[1]*result.shape[2]*result.shape[3])\n",
    "        c_file.write('\\t twoD_t meta_data'+str(index)+' = {\\n\\\n",
    "                           .r = '+str(result.shape[2])+',\\n\\\n",
    "                           .c = '+str(result.shape[3])+',\\n\\\n",
    "                           .channel = '+str(result.shape[1])+',\\n\\\n",
    "                           .data = buffer'+str(index%2)+',\\n\\\n",
    "                           .bias = NULL\\n\\\n",
    "                       };\\n\\n')        \n",
    "        meta_list.append(('meta_data'+str(index),(i.stride, i.kernel_size, previous_padding)))\n",
    "    \n",
    "    if 'Linear' in str(i):\n",
    "        index += 1\n",
    "        #print(result.shape)\n",
    "        L = np.append(L, result.shape[1])\n",
    "        c_file.write('\\t twoD_t meta_data'+str(index)+' = {\\n\\\n",
    "                           .r = '+str(result.shape[1]) +',\\n\\\n",
    "                           .c = 1,\\n\\\n",
    "                           .channel = 1,\\n\\\n",
    "                           .data = buffer'+str(index%2)+',\\n\\\n",
    "                           .bias = NULL\\n\\\n",
    "                       };\\n\\n')\n",
    "        meta_list.append(('meta_data'+str(index),(0,0,0)))\n",
    "\n",
    "c_file.write('\\n\\t memcpy(buffer0, test, sizeof(test));\\n')\n",
    "\n",
    "c_file.write('\\n\\t printf(\"---Network starts---\\\\n\");\\n')\n",
    "\n",
    "\n",
    "inx = np.argsort(L)\n",
    "\n",
    "weights_file.write('\\n\\\n",
    "data_t buffer'+str(inx[-1]%2)+'['+str(int(L[inx[-1]]))+'];\\n\\\n",
    "data_t buffer'+str((inx[-1]+1)%2)+'['+str(int(L[inx[-2]]))+'];\\n\\\n",
    "\\n\\\n",
    "typedef struct twoD\\n\\\n",
    "{\\n\\\n",
    "\tuint32_t r;\\n\\\n",
    "\tuint32_t c;\\n\\\n",
    "\tuint32_t in_channel;\\n\\\n",
    "\tuint32_t channel;\\n\\\n",
    "\tdata_t *data;\\n\\\n",
    "\tdata_t *bias;\\n\\\n",
    "} twoD_t;\\n\\n')\n",
    "\n",
    "prev_shapes = None\n",
    "prev_arr_name = None\n",
    "\n",
    "index = 0\n",
    "for name, param in best_model.state_dict().items():\n",
    "\n",
    "    arr = param.cpu().numpy();\n",
    "    shape_of_params = arr.shape\n",
    "    param_size =  len(arr.flatten())\n",
    "    #print(name, 'param size:', param_size)\n",
    "    underscore_arr_name = 'w_'+name.replace('.', '_')\n",
    "    array_name  = underscore_arr_name + '[' + str(param_size) + ']=' \n",
    "    weights_file.write('const data_t '+array_name+'{')\n",
    "    np.savetxt(weights_file, arr.flatten(), newline=',')\n",
    "    weights_file.write('};\\n\\n')\n",
    "\n",
    "    if len(shape_of_params) == 1:\n",
    "        index += 1\n",
    "        if len(prev_shapes) > 2:\n",
    "            print('Conv layer')\n",
    "            out_channel = prev_shapes[0]\n",
    "            in_channel = prev_shapes[1]\n",
    "            c_file.write('\\t conv2D(&'+meta_list[index-1][0]+', &'+prev_arr_name+'_2d, &'+meta_list[index][0]+', &reLU, '+str(meta_list[index][1][0])+', '+str(meta_list[index][1][1])+', '+str(meta_list[index][1][1])+', '+str(meta_list[index][1][2])+');\\n')\n",
    "            \n",
    "        else:\n",
    "            print('linear layer')\n",
    "            out_channel = 1\n",
    "            in_channel = 1\n",
    "            if len(meta_list) == (index+1):\n",
    "                c_file.write('\\t dot(&'+meta_list[index-1][0]+', &'+prev_arr_name+'_2d, &'+meta_list[index][0]+', NULL);\\n')\n",
    "            else:\n",
    "                c_file.write('\\t dot(&'+meta_list[index-1][0]+', &'+prev_arr_name+'_2d, &'+meta_list[index][0]+', &reLU);\\n')\n",
    "                \n",
    "        weights_file.write('const twoD_t '+prev_arr_name+'_2d = {\\n\\\n",
    "                           .r = '+ str(prev_shapes[-1]) +',\\n\\\n",
    "                           .c = '+str(prev_shapes[-2])+',\\n\\\n",
    "                           .in_channel = '+str(in_channel)+',\\n\\\n",
    "                           .channel = '+str(out_channel)+',\\n\\\n",
    "                           .data = '+prev_arr_name+',\\n\\\n",
    "                           .bias = '+underscore_arr_name+'\\n\\\n",
    "                       };\\n\\n')\n",
    "        print(prev_arr_name+'_2d', prev_shapes)\n",
    "\n",
    "    prev_shapes = shape_of_params\n",
    "    prev_arr_name = underscore_arr_name\n",
    "\n",
    "c_file.write('\\n\\t print_twoD(&'+meta_list[-1][0]+', 0);\\n')\n",
    "c_file.write('\\t printf(\"PREDICTION: %d\\\\n\", get_class(&'+meta_list[-1][0]+'));\\n')\n",
    "\n",
    "class network_partial(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(network_partial, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-6])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "intermediate_network= network_partial(best_model)\n",
    "\n",
    "weights_file.close()\n",
    "\n",
    "c_file.write('\\t return 0;\\n}')\n",
    "\n",
    "c_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
